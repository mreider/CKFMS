groups:
  - id: gen_ai
    type: span
    brief:
    attributes:
      - id: gen_ai.operation.name
        type: string
        stability: experimental
        brief: Name of operation being performed.
        examples: ["chat", "generate_content", "text_completion"]

      - id: gen_ai.provider.name
        type: string
        stability: experimental
        brief: Name of GenAI product being used.
        examples: ["aws_bedrock", "openai"]

      - id: gen_ai.request.model
        type: string
        stability: experimental
        brief: Model chosen to handle the request.
        examples: ["amazon.nova-micro-v1:0", "anthropic.claude-3-7-sonnet-20250219-v1:0"]

      - id: gen_ai.response.model
        type: string
        stability: experimental
        brief: Model that handled the request.
        examples: ["amazon.nova-micro-v1:0", "anthropic.claude-3-7-sonnet-20250219-v1:0"]

      - id: gen_ai.usage.input_tokens
        type: long
        stability: experimental
        brief: Number of tokens sent to the model in the request.
        examples: 42

      - id: gen_ai.usage.output_tokens
        type: long
        stability: experimental
        brief: Number of tokens generated by the model while handling the request.
        examples: 42

      - id: gen_ai.guardrail.id
        type: string
        stability: experimental
        brief: Identifier of the guardrail that has been activated for the request.
        examples: "sensitive_data_guardrail"

      - id: gen_ai.guardrail.version
        type: string
        stability: experimental
        brief: Version of the guardrail that has been activated.
        examples: ["DRAFT", "5", "12345678"]

      - id: gen_ai.prompt_caching
        type:
          allow_custom_values: true
          members:
            - id: read
              value: "read"
              brief: Cache hit. Reading from cache.
            - id: write
              value: "write"
              brief: Cache miss. Creating cache checkpoint.

        stability: experimental
        brief: Indicates how prompt cache has been used when handling the request.
        examples: ["read", "write"]

      - id: gen_ai.request.frequency_penalty
        type: double
        stability: experimental
        brief: Frequency penalty setting for GenAI request.
        examples: 0.4

      - id: gen_ai.request.presence_penalty
        type: double
        stability: experimental
        brief: Presence penalty setting for GenAI request.
        examples: 0.4

      - id: gen_ai.request.max_tokens
        type: long
        stability: experimental
        brief: Maximum number of tokens that the model can generate for a request.
        examples: 50

      - id: gen_ai.request.stop_sequences
        type: string[]
        stability: experimental
        brief: List of sequences that will stop the model from generating further tokens.
        examples: ["forest", "lived"]

      - id: gen_ai.request.temperature
        type: double
        stability: experimental
        brief: Temperature setting for GenAI request.
        examples: 0.8

      - id: gen_ai.request.top_k
        type: long
        stability: experimental
        brief: Temperature setting for GenAI request.
        examples: 300

      - id: gen_ai.request.top_p
        type: double
        stability: experimental
        brief: Temperature setting for GenAI request.
        examples: 0.6

      - id: gen_ai.response.finish_reasons
        type: string[]
        stability: experimental
        brief: List of reasons why the model stopped generating tokens, corresponding to each generation received.
        examples: ['["stop_sequence"]', '["stop_sequence", "max_tokens"]']
